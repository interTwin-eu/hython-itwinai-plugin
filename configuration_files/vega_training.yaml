# --------------------------------------------------------------------------------------
# Part of the interTwin Project: https://www.intertwin.eu/
#
# Created by: Matteo Bunino
#
# Credit:
# - Jarl Sondre SÃ¦ther <jarl.sondre.saether@cern.ch> - CERN
# - Henry Mutegeki <henry.mutegeki@cern.ch> - CERN
# - Iacopo Ferrario <iacopofederico.ferrario@eurac.edu> - EURAC
# - Matteo Bunino <matteo.bunino@cern.ch> - CERN
# - Linus Maximilian Eickhoff <linus.maximilian.eickhoff@cern.ch> - CERN
# --------------------------------------------------------------------------------------

#! change run or exp name for each user (overwrite will fail due to permissions)
# General configuration
experiment_name: "vega-train-eurac"
run_name: "vega-train-eurac-1"

work_dir: /ceph/hpc/data/st2301-itwin-users/eurac/model

train_temporal_range: ["2017-01-01", "2019-12-31"]
valid_temporal_range: ["2020-01-01", "2020-12-31"]
test_temporal_range: ["2022-01-01", "2022-12-31"]

seed: 10 

# === Model ===

model: CudaLSTM

hidden_size: 128
dropout: 0.2
lstm_layers: 1
lstm_batch_norm: False

model_head_layer: regression # distr_normal
model_head_activation: linear
model_head_kwargs: {}

find_unused_parameters: False
  
# === Training ===

strategy: ddp

hython_trainer: rnntrainer

loss_fn:
  _target_: hython.losses.RMSELoss

optimizer: adam

lr_scheduler: null # needed in TrainingConfiguration
lr_scheduler_hython:
  mode: min
  factor: 0.5
  patience: 10

seq_length: 30

learning_rate: 0.001

batch: 512

epochs: 5
time_ray: True

gradient_clip:
    max_norm: 1

target_weights: even # null, even, or dict

# which steps are used in the computation of the loss function
predict_steps: 0 # all # (prediction: 0 = t ), ( forecasts: 1 = t+1, 2 = t + 2)


# > Donwsampling < 

# static 
train_downsampler: #null
  _target_: hython.sampler.downsampler.RandomDownsampler
  frac_time: null
  frac_space: 0.1

valid_downsampler: #null
  _target_: hython.sampler.downsampler.RandomDownsampler
  frac_time: null
  frac_space: 0.1

test_downsampler: null


#  dynamic 
dynamic_downsampler:
    frac_time: 0.1

# === Data ===
data_lazy_load: True
dataset: WflowSBM_HPC
num_workers_dataloader: 8

# === Ray HPO ===
num_workers_per_trial: 2
cpu_per_worker: 11
gpu_per_worker: 1 # 0. - 1
trials: 2


data_source:
  file:
    static_inputs: /ceph/hpc/data/st2301-itwin-users/eurac/eobs_static.zarr
    dynamic_inputs: /ceph/hpc/data/st2301-itwin-users/eurac/eobs_dynamic.zarr
    target_variables: /ceph/hpc/data/st2301-itwin-users/eurac/input/eobs_dynamic.zarr
# s3:
  #   url: https://eurac-eo.s3.amazonaws.com/INTERTWIN/SURROGATE_INPUT/adg1km_eobs_preprocessed.zarr/

static_categorical_inputs:
   - wflow_landuse
   - wflow_soil

static_inputs:
  - thetaR
  - KsatVer
  - c

dynamic_inputs:
  - precip
  - pet
  - temp
  
target_variables:
  - vwc

mask_variables:
  - mask_missing
  - mask_lake

# Preprocess

preprocessor:
    # static_inputs:
    #     lazy: False
    #     variant:
    #       - _target_: hython.preprocessor.Log10p
    #         variable:
    #             - "KsatVer"
    dynamic_inputs:
        lazy: False
        variant:
          - _target_: hython.preprocessor.Log10p
            variable:
                - "precip"

# Scaling 

scaling_use_cached: False
scaling_static_range:
    thetaS: [0.25, 0.95] 
    thetaR: [0.01, 0.35] # it was 0.25 in table
    KsatVer: [1, 10000]
    c: [1, 20]
    f: [0.00001, 0.1] # from training data (it is fitted)
    M: [1, 3000]
    RootingDepth: [100, 2000 ] #5000] # (mm)
    Swood: [0.0, 0.5] # -
    Sl: [0.02, 0.2] #(mm)
    Kext: [0.48, 0.96] # 


scaler:
    static_inputs:
        lazy: False
        variant:
          - _target_: hython.scaler.BoundedScaler
            variable:
              thetaR: ${scaling_static_range.thetaR}
              KsatVer: ${scaling_static_range.KsatVer}
              c: ${scaling_static_range.c}
    dynamic_inputs:
        lazy: False
        variant:
          - _target_: hython.scaler.MinMax01Scaler
            variable:
              - "pet"
              - "precip"
          - _target_: hython.scaler.StandardScaler
            variable:
              - "temp"
    target_variables:
        lazy: False
        variant:
          - _target_: hython.scaler.MinMax01Scaler
            variable: ${target_variables}

# MLFLow
tracking_uri: null # http://mlflow.intertwin.fedcloud.eu/

model_logger:
  CudaLSTM:
      logger: local
      model_component: model # the main model
      model_name: ${model}
      model_uri: ${work_dir}/${experiment_name}_${run_name}/${model}.pt
      log: True
      load: False

training:
  _target_: itwinai.pipeline.Pipeline
  steps:
    - _target_: itwinai.plugins.hython.data.RNNDatasetGetterAndPreprocessor
      hython_trainer: ${hython_trainer}
      dynamic_inputs: ${dynamic_inputs}
      static_inputs: ${static_inputs}
      target_variables: ${target_variables}
      mask_variables: ${mask_variables}
      train_temporal_range: ${train_temporal_range}
      valid_temporal_range: ${valid_temporal_range}
      dataset: ${dataset}
      data_lazy_load: ${data_lazy_load}
      data_source: ${data_source}
      preprocessor: ${preprocessor}
      scaler: ${scaler}
      scaling_use_cached: ${scaling_use_cached}
      experiment_name: ${experiment_name}
      work_dir: ${work_dir}
      train_downsampler: ${train_downsampler}
      valid_downsampler: ${valid_downsampler}
      seq_length: ${seq_length}
    - _target_: itwinai.plugins.hython.trainer.RNNDistributedTrainer
      model: ${model}
      config:
        experiment: ${experiment_name}/${run_name}
        experiment_name: ${experiment_name}
        work_dir: ${work_dir}
        batch_size: ${batch}
        learning_rate: ${learning_rate}
        num_workers_dataloader: ${num_workers_dataloader}
        pin_gpu_memory: True
        hython_trainer: ${hython_trainer}
        dynamic_downsampler: ${dynamic_downsampler}
        seq_length: ${seq_length}
        target_variables: ${target_variables}
        dynamic_inputs: ${dynamic_inputs}
        static_inputs: ${static_inputs}

        optimizer: ${optimizer}
        lr_scheduler: ${lr_scheduler}
        target_weights: ${target_weights}

        # model config
        hidden_size: ${hidden_size}
        dropout: ${dropout}
        lstm_layers: ${lstm_layers}
        lstm_batch_norm: ${lstm_batch_norm}

        model_head_layer: ${model_head_layer}
        model_head_activation: ${model_head_activation}
        model_head_kwargs: ${model_head_kwargs}

        loss_fn: ${loss_fn}

        gradient_clip: ${gradient_clip}

        predict_steps: ${predict_steps}

        # model logger
        model_logger: ${model_logger}
        
      run_name: ${run_name}
      strategy: ${strategy}
      epochs: ${epochs}
      metrics:
        mse:
          _target_: torchmetrics.regression.MeanSquaredError
      measure_gpu_data: True
      measure_epoch_time: True
      enable_torch_profiling: False
      random_seed: ${seed}
      profiling_wait_epochs: 1
      profiling_warmup_epochs: 1
      logger:
        _target_: itwinai.loggers.LoggersCollection
        loggers:
          - _target_: itwinai.loggers.ConsoleLogger
            log_freq: 1
          - _target_: itwinai.loggers.MLFlowLogger
            experiment_name: ${experiment_name}
            run_name: ${run_name}
            log_on_workers: -1
            log_freq: epoch
            tracking_uri: ${tracking_uri}

hpo:
  _target_: itwinai.pipeline.Pipeline
  steps:
    - _target_: itwinai.plugins.hython.data.RNNDatasetGetterAndPreprocessor
      hython_trainer: ${hython_trainer}
      dynamic_inputs: ${dynamic_inputs}
      static_inputs: ${static_inputs}
      target_variables: ${target_variables}
      mask_variables: ${mask_variables}
      train_temporal_range: ${train_temporal_range}
      valid_temporal_range: ${valid_temporal_range}
      dataset: ${dataset}
      data_lazy_load: ${data_lazy_load}
      data_source: ${data_source}
      preprocessor: ${preprocessor}
      scaler: ${scaler}
      scaling_use_cached: ${scaling_use_cached}
      experiment_name: ${experiment_name}
      work_dir: ${work_dir}
      train_downsampler: ${train_downsampler}
      valid_downsampler: ${valid_downsampler}
      seq_length: ${seq_length}
    - _target_: itwinai.plugins.hython.trainer.RNNDistributedTrainer
      model: ${model}
      config:
        experiment: ${experiment_name}/${run_name}
        experiment_name: ${experiment_name}
        work_dir: ${work_dir}
        batch_size: ${batch}
        learning_rate: ${learning_rate}
        num_workers_dataloader: ${num_workers_dataloader}
        pin_gpu_memory: True
        hython_trainer: ${hython_trainer}
        dynamic_downsampler: ${dynamic_downsampler}
        seq_length: ${seq_length}
        target_variables: ${target_variables}
        dynamic_inputs: ${dynamic_inputs}
        static_inputs: ${static_inputs}

        optimizer: ${optimizer}
        lr_scheduler: ${lr_scheduler}
        target_weights: ${target_weights}

        # model config


        hidden_size: ${hidden_size}
        dropout: ${dropout}
        lstm_layers: ${lstm_layers}
        lstm_batch_norm: ${lstm_batch_norm}

        model_head_layer: ${model_head_layer}
        model_head_activation: ${model_head_activation}
        model_head_kwargs: ${model_head_kwargs}

        loss_fn: ${loss_fn}
        gradient_clip: ${gradient_clip}

        predict_steps: ${predict_steps}

        # model logger
        model_logger: ${model_logger}
        
      run_name: ${run_name}
      time_ray: ${time_ray}
      strategy: ${strategy}
      epochs: ${epochs}
      metrics:
        mse:
          _target_: torchmetrics.regression.MeanSquaredError
      measure_gpu_data: True
      measure_epoch_time: True
      enable_torch_profiling: False
      random_seed: ${seed}
      profiling_wait_epochs: 1
      profiling_warmup_epochs: 1
      logger:
        _target_: itwinai.loggers.LoggersCollection
        loggers:
          - _target_: itwinai.loggers.ConsoleLogger
            log_freq: 1
          - _target_: itwinai.loggers.MLFlowLogger
            experiment_name: ${experiment_name}
            run_name: ${run_name}
            log_on_workers: -1
            log_freq: epoch
            tracking_uri: ${tracking_uri}
                    
      ray_scaling_config:
        _target_: ray.train.ScalingConfig
        num_workers: ${num_workers_per_trial}
        use_gpu: true
        resources_per_worker:
          CPU: ${cpu_per_worker}
          GPU: ${gpu_per_worker}
      ray_tune_config:
        _target_: ray.tune.TuneConfig
        num_samples: ${trials}
        scheduler: null
        #   _target_: ray.tune.schedulers.ASHAScheduler
        #   metric: loss
        #   mode: min
        #   max_t: 5
        #   grace_period: 2
        #   reduction_factor: 6
        #   brackets: 1
      ray_run_config:
        _target_: ray.tune.RunConfig
        # storage_path must be an absolute path. Defaulting to the directory from which the
        # job is launched using the itwinai custom OmegaConf resolver ${itwinai.cwd:}
        storage_path: ${itwinai.cwd:}/ray_checkpoints
        name: EURAC-HPO-Experiment
      ray_search_space:
        batch_size:
          type: qrandint
          lower: 128
          upper: 1024
          q: 128
        optim_lr:
          type: uniform
          lower: 1e-4
          upper: 1e-2
        hidden_size:
          type: randint
          lower: 16
          upper: 512
        dropout:
          type: choice
          categories: [0.1, 0.2, 0.3, 0.4, 0.5]
        seq_length:
          type: qrandint
          lower: 90
          upper: 365
          q: 30
        
