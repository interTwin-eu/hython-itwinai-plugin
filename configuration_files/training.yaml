# --------------------------------------------------------------------------------------
# Part of the interTwin Project: https://www.intertwin.eu/
#
# Created by: Matteo Bunino
#
# Credit:
# - Jarl Sondre SÃ¦ther <jarl.sondre.saether@cern.ch> - CERN
# - Henry Mutegeki <henry.mutegeki@cern.ch> - CERN
# - Iacopo Ferrario <iacopofederico.ferrario@eurac.edu> - EURAC
# - Matteo Bunino <matteo.bunino@cern.ch> - CERN
# --------------------------------------------------------------------------------------

# General configuration
experiment_name: "training"
#! change run name for each user on JSC (overwrite will fail due to permissions)
experiment_run: "hython-plugin3"

work_dir: /p/scratch/intertwin/datasets/eurac/model

train_temporal_range: ["2016-01-01", "2019-12-31"]
valid_temporal_range: ["2020-01-01", "2020-12-31"]
test_temporal_range: ["2019-01-01", "2020-12-31"]

seed: 10 

device: "cuda:0"

# === Model ===

model: CudaLSTM

hidden_size: 64
dropout: 0.2
lstm_layers: 1
lstm_batch_norm: False

model_head_layer: regression # distr_normal
model_head_activation: linear
model_head_kwargs: {}

find_unused_parameters: False
  
# === Training ===

strategy: ddp

hython_trainer: rnntrainer

loss_fn:
  _target_: hython.losses.RMSELoss
    
metric_fn:
  _target_: hython.metrics.MSEMetric

optimizer: adam

lr_scheduler: null # needed in TrainingConfiguration
lr_scheduler_hython:
  mode: min
  factor: 0.5
  patience: 10

seq_length: 180

learning_rate: 0.001

batch: 512

epochs: 2

gradient_clip: null

target_weights: even # null, even, or dict

# which steps are used in the computation of the loss function
predict_steps: 0 # all # (prediction: 0 = t ), ( forecasts: 1 = t+1, 2 = t + 2)


# > Donwsampling < 

# spatial downsampling
train_downsampler: #null
  _target_: hython.sampler.downsampler.RandomDownsampler
  frac_time: null
  frac_space: 0.01

valid_downsampler: #null
  _target_: hython.sampler.downsampler.RandomDownsampler
  frac_time: null
  frac_space: 0.01

test_downsampler: null


# temporal downsampling
downsampling_temporal_dynamic: True
temporal_downsampling: True
temporal_subset: [50, 50]


# === Data ===

data_lazy_load: False

dataset: WflowSBM

data_source:
  file:
    static_inputs: /p/scratch/intertwin/datasets/eurac/input/eobs_static.zarr
    dynamic_inputs: /p/scratch/intertwin/datasets/eurac/input/eobs_dynamic.zarr
    target_variables: /p/scratch/intertwin/datasets/eurac/input/eobs_dynamic.zarr
  # s3:
  #   url: https://eurac-eo.s3.amazonaws.com/INTERTWIN/SURROGATE_INPUT/adg1km_eobs_preprocessed.zarr/

static_categorical_inputs:
   - wflow_landuse
   - wflow_soil

static_inputs:
  - thetaS
  - thetaR
  - KsatVer
  - c

dynamic_inputs:
  - precip
  - pet
  - temp
  
target_variables:
  - vwc

mask_variables:
  - mask_missing
  - mask_lake

# Scaling

scaling_variant: minmax
scaling_use_cached: True

# ==== MODEL LOGGER ========
#mlflow: 
# - ``/Users/me/path/to/local/model``
# - ``relative/path/to/local/model``
# - ``s3://my_bucket/path/to/model``
# - ``runs:/<mlflow_run_id>/run-relative/path/to/model``
# - ``models:/<model_name>/<model_version>``
# - ``models:/<model_name>/<stage>``
#local:
# "/home/iferrario/dev/notebooks/config/lstm_v2.yaml"
# /mnt/CEPH_PROJECTS/InterTwin/hython_model_run/lstm_vwc/model.pt

# where the model is loaded/saved
model_logger:
  CudaLSTM:
      logger: local
      model_component: model # the main model
      model_name: ${model}
      model_uri: ${work_dir}/${experiment_name}_${experiment_run}/${model}.pt
      #model_uri: "models:/${model}/latest" #  
      log: True
      load: False



# == Pipeline == 

training_pipeline:
  _target_: itwinai.pipeline.Pipeline
  steps:
    - _target_: itwinai.plugins.hython.data.RNNDatasetGetterAndPreprocessor
      hython_trainer: ${hython_trainer}
      dynamic_inputs: ${dynamic_inputs}
      static_inputs: ${static_inputs}
      target_variables: ${target_variables}
      mask_variables: ${mask_variables}
      train_temporal_range: ${train_temporal_range}
      valid_temporal_range: ${valid_temporal_range}
      dataset: ${dataset}
      data_lazy_load: ${data_lazy_load}
      downsampling_temporal_dynamic: ${downsampling_temporal_dynamic}
      data_source: ${data_source}
      scaling_variant: ${scaling_variant}
      scaling_use_cached: ${scaling_use_cached}
      experiment_name: ${experiment_name}
      experiment_run: ${experiment_run}
      work_dir: ${work_dir}
      train_downsampler: ${train_downsampler}
      valid_downsampler: ${valid_downsampler}
    - _target_: itwinai.plugins.hython.trainer.RNNDistributedTrainer
      model: ${model}
      config:
        experiment: ${experiment_name}/${experiment_run}
        experiment_name: ${experiment_name}
        experiment_run: ${experiment_run}
        work_dir: ${work_dir}
        batch_size: ${batch}
        learning_rate: ${learning_rate}
        num_workers_dataloader: 1
        hython_trainer: ${hython_trainer}
        #temporal_downsampling_dynamic: ${downsampling_temporal_dynamic}
        temporal_downsampling: ${temporal_downsampling}
        temporal_subset: ${temporal_subset}
        seq_length: ${seq_length}
        target_variables: ${target_variables}
        dynamic_inputs: ${dynamic_inputs}
        static_inputs: ${static_inputs}

        optimizer: ${optimizer}
        lr_scheduler: ${lr_scheduler}
        target_weights: ${target_weights}

        # model config


        hidden_size: ${hidden_size}
        dropout: ${dropout}
        lstm_layers: ${lstm_layers}
        lstm_batch_norm: ${lstm_batch_norm}

        model_head_layer: ${model_head_layer}
        model_head_activation: ${model_head_activation}
        model_head_kwargs: ${model_head_kwargs}

        loss_fn: ${loss_fn}
        metric_fn: ${metric_fn}

        gradient_clip: ${gradient_clip}

        predict_steps: ${predict_steps}

        # model logger
        model_logger: ${model_logger}

      strategy: ${strategy}
      epochs: ${epochs}
      random_seed: ${seed}
      profiling_wait_epochs: 1
      profiling_warmup_epochs: 1
      logger:
        _target_: itwinai.loggers.LoggersCollection
        loggers:
          - _target_: itwinai.loggers.ConsoleLogger
            log_freq: 1
          # - _target_: itwinai.loggers.MLFlowLogger
          #   experiment_name: ${experiment_name}
          #   run_name: ${experiment_run}
          #   log_freq: batch
          #   savedir: /home/iferrario/dev/hython/hython/itwinai/mllogs
                    
      ray_scaling_config:
        _target_: ray.train.ScalingConfig
        num_workers: 1
        use_gpu: true
        resources_per_worker:
          CPU: 2
          GPU: 1
      ray_tune_config:
        _target_: ray.tune.TuneConfig
        num_samples: 2
        scheduler:
          _target_: ray.tune.schedulers.ASHAScheduler
          metric: loss
          mode: min
          max_t: 5
          grace_period: 2
          reduction_factor: 6
          brackets: 1
      ray_run_config:
        _target_: ray.train.RunConfig
        # storage_path must be an absolute path. Defaulting to the directory from which the
        # job is launched using the itwinai custom OmegaConf resolver ${itwinai.cwd:}
        storage_path: ${itwinai.cwd:}/ray_checkpoints
        name: MNIST-HPO-Experiment
      ray_search_space:
        batch_size:
          type: choice
          categories: [256, 512]
        optim_lr:
          type: uniform
          lower: 1e-5
          upper: 1e-3